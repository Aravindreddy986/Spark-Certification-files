----------- list tables and list databases -------

sqoop list-databases \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306" \
  --username retail_dba \
  --password hadoop
  
 sqoop list-databases \
  --connect jdbc:mysql://localhost:3306 \
  --username retail_dba \
  --password hadoop

sqoop list-tables \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username retail_dba \
  --password hadoop

sqoop list-tables \
  --connect "jdbc:mysql://localhost:3306/retail_db" \
  --username retail_dba \
  --password hadoop
   
 --------------------- eval command ---------------------------------
 
sqoop eval \
  --connect jdbc:mysql://localhost:3306/retail_db \
  --username retail_dba \
  --password hadoop \
  --query "select count(1) from order_items"
  
  sqoop eval \
  --connect jdbc:mysql://localhost:3306/retail_db \
  --username retail_user \
  --password cloudera \
  --query "select * from order_items LIMIT 10"

sqoop eval \
  --connect jdbc:mysql://localhost:3306/retail_db \
  --username retail_user \
  --password cloudera \
  --query "insert into order values(\"2017-10-31\"....)" use single quotes or incase of double quotes use escape charecter \
  
-- Reference: http://www.hadoop.com/content/hadoop/en/developers/home/developer-admin-resources/get-started-with-hadoop-tutorial/exercise-1.html

------------------- Import commands ------------------------

sqoop import
 --connect jdbc:mysql://localhost:3306/retail_db \
  --username retail_user \
  --password cloudera \
  --table order_items
  --target-dir /user/cloudera/sqoop-import/retail_db/order_items

sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db

hadoop fs -tail /user/cloudera/sqoop-import/retail_db/order_items
hadoop fs -rm -R /user/cloudera/sqoop-import/retail_db/order_items

sqoop import \
  --connect "jdbc:mysql://hiveserver:3306/retail_db" \
  --username=retail_dba \
  --password=itversity \
  --table departments \
  --as-textfile \
  --target-dir=/user/root/departments


------------ execution life cycle --------------------------
Map reduce code
Need to uderstand the structure of the table
once the structure is identified, generates a java code with the table object

sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db
  --num-mappers 1
  
-------------- Managing directories --------------------

sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db
  --num-mappers 1
  --delete-target-dir
  
 sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db
  --num-mappers 1
  --append

table need to have primary column, what if table does not have primary key
things to remember when using split-by, column needs to be indexed, 
values in the column should be spare and
often it should be sequence generated
the column should not have null values
 -- Number of mappers, Splitting using custom columns and splitting using non numeric columns
 
 sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db
  --split-by columname

 sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db
  --num-mappers 1
  
 sqoop import \
  -Dorg.apache.sqoop.splitter.allow_text_splitter=true
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db
  --split-by columname
  
 reset the number of mappers to one if there is no primary key on the table
  sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db
  --autoreset-to-one-mapper
  
 ---------------- different file formats -----------------------
 ---- Text file, Sequence file, Avro file, Parquet file
 
  sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db
  --num-mappers 2
  --as-avrofile
  --as-sequencefile
  --as-textfile
  --as-parquetfile
  
 ------------ Compresion algorithm
 -- Gzip, Deflate, Snappy, Others

  sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db
  --num-mappers 2
  --as-textfile
  --compress
  
 in core-site.xml search for compress
   
  sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db
  --num-mappers 2
  --as-textfile
  --compress
  --compression-codec org.apache.hadoop.io.compress.SnappyCodec

------------------ boundary query--------------

  sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db
  --bounday-query 'select min(orderid), max(orderid) from tablename where orderid >= ?'

hardcoding the range
sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db
  --bounday-query 'select 10000, 172998'

table and column is mutually exclusive with query
remember to use \$conditions in the where clause
#for query split-by is mandatory if num-mappers is greater than 1
sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --columns orderid, column2, column3 \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db
  
sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db \
  --num-mappers 2 \
  --query 'select query '
  --split-by orderid
  
------- Delimiters and handling nulls -----------------------------

sqoop import \
  --connect jdbc:mysql://ms.itversity.com:3306/retail_db \
  --username retail_user \
  --password itversity \
  --table order_items \
  --warehouse-dir /user/dgadiraju/sqoop_import/retail_db
  --null-non-string -1
  --fields-terminated-by "\t" \
  --lines-terminated-by ":"
  
  
--------------------------------------------------
sqoop import-all-tables \
  --connect "jdbc:mysql://cdh54itversity-mn0.southeastasia.cloudapp.azure.com:3306/retail_db" \
  --username retail_dba \
  --password itversity \
  --warehouse-dir /user/itversity/dgadiraju/import-all-tables \
  --driver com.mysql.jdbc.Driver

sqoop import-all-tables \
  -m 12 \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --as-avrodatafile \
  --warehouse-dir=/apps/hive/warehouse/retail_stage.db

--Default
sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --as-textfile \
  --target-dir=/user/root/departments

sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --as-sequencefile \
  --target-dir=/user/root/departments

sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --as-avrodatafile \
  --target-dir=/user/root/departments

-- A file with extension avsc will be created under the directory from which sqoop import is executed
-- Copy avsc file to HDFS location
-- Create hive table with LOCATION to /user/root/departments and TBLPROPERTIES pointing to avsc file
hadoop fs -put sqoop_import_departments.avsc /user/root

CREATE EXTERNAL TABLE departments
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe'
STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'
LOCATION 'hdfs:///user/root/departments'
TBLPROPERTIES ('avro.schema.url'='hdfs://sandbox.hortonworks.com/user/root/sqoop_import_departments.avsc');


-- It will create tables in default database in hive
-- Using snappy compression
-- As we have imported all tables before make sure you drop the directories
-- Launch hive drop all tables
drop table departments;
drop table categories;
drop table products;
drop table orders;
drop table order_items;
drop table customers;

-- Dropping directories, in case your hive database/tables in consistent state
hadoop fs -rm -R /apps/hive/warehouse/departments
hadoop fs -rm -R /apps/hive/warehouse/categories
hadoop fs -rm -R /apps/hive/warehouse/products
hadoop fs -rm -R /apps/hive/warehouse/orders 
hadoop fs -rm -R /apps/hive/warehouse/order_itmes
hadoop fs -rm -R /apps/hive/warehouse/customers

sqoop import-all-tables \
  --num-mappers 1 \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --hive-import \
  --hive-overwrite \
  --create-hive-table \
  --compress \
  --compression-codec org.apache.hadoop.io.compress.SnappyCodec \
  --outdir java_files

sudo -u hdfs hadoop fs -mkdir /user/root/retail_stage
sudo -u hdfs hadoop fs -chmod +rw /user/root/retail_stage
hadoop fs -copyFromLocal ~/*.avsc /user/root/retail_stage

-- Basic import
sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --target-dir /user/root/departments 

-- Boundary Query and columns
sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --target-dir /user/root/departments \
  -m 2 \
  --boundary-query "select 2, 8 from departments limit 1" \
  --columns department_id,department_name

-- query and split-by
sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --query="select * from orders join order_items on orders.order_id = order_items.order_item_order_id where \$CONDITIONS" \
  --target-dir /user/root/order_join \
  --split-by order_id \
  --num-mappers 4

-- Copying into existing table or directory (append)
-- Customizing number of threads (num-mappers)
-- Changing delimiter
sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --target-dir /apps/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --num-mappers 1 \
  --outdir java_files

-- Importing table with out primary key using multiple threads (split-by)
-- When using split-by, using indexed column is highly desired
-- If the column is not indexed then performance will be bad 
-- because of full table scan by each of the thread
sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --target-dir /apps/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --split-by department_id \
  --outdir java_files

-- Getting delta (--where)
sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --target-dir /apps/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --split-by department_id \
  --where "department_id > 7" \
  --outdir java_files

-- Incremental load
sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --target-dir /apps/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --check-column "department_id" \
  --incremental append \
  --last-value 7 \
  --outdir java_files

sqoop job --create sqoop_job \
  -- import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --target-dir /apps/hive/warehouse/retail_ods.db/departments \
  --append \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --check-column "department_id" \
  --incremental append \
  --last-value 7 \
  --outdir java_files

sqoop job --list

sqoop job --show sqoop_job

sqoop job --exec sqoop_job

-- Hive related
-- Overwrite existing data associated with hive table (hive-overwrite)
sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --hive-home /apps/hive/warehouse/retail_ods.db \
  --hive-import \
  --hive-overwrite \
  --hive-table departments \
  --outdir java_files

sqoop import \
  --connect "jdbc:mysql://<host_name_here>:3306/retail_db" \
  --username=retail_dba \
  --password=itversity \
  --table departments \
  --hive-home /user/hive/warehouse \
  --hive-import \
  --hive-table sqoop_demo.departments_dg \
  --create-hive-table

--Create hive table example
sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --fields-terminated-by '|' \
  --lines-terminated-by '\n' \
  --hive-home /apps/hive/warehouse \
  --hive-import \
  --hive-table departments_test \
  --create-hive-table \
  --outdir java_files

--Connect to mysql and create database for reporting database
--user:root, password:hadoop
mysql -u root -p
create database retail_rpt_db;
grant all on retail_rpt_db.* to retail_dba;
flush privileges;
use retail_rpt_db;
create table departments as select * from retail_db.departments where 1=2;
exit;

--For certification change database name retail_rpt_db to retail_db
sqoop export --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_rpt_db" \
       --username retail_dba \
       --password hadoop \
       --table departments \
       --export-dir /apps/hive/warehouse/retail_ods.db/departments \
       --input-fields-terminated-by '|' \
       --input-lines-terminated-by '\n' \
       --num-mappers 2 \
       --batch \
       --outdir java_files

sqoop export --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username retail_dba \
  --password hadoop \
  --table departments \
  --export-dir /user/root/sqoop_import/departments_export \
  --batch \
  --outdir java_files \
  -m 1 \
  --update-key department_id \
  --update-mode allowinsert

sqoop export --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username retail_dba \
  --password hadoop \
  --table departments_test \
  --export-dir /apps/hive/warehouse/departments_test \
  --input-fields-terminated-by '\001' \
  --input-lines-terminated-by '\n' \
  --num-mappers 2 \
  --batch \
  --outdir java_files \
  --input-null-string nvl \
  --input-null-non-string -1

--Merge process begins
hadoop fs -mkdir /user/root/sqoop_merge

--Initial load
sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --as-textfile \
  --target-dir=/user/root/sqoop_merge/departments

--Validate
sqoop eval --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username retail_dba \
  --password hadoop \
  --query "select * from departments" 

hadoop fs -cat /user/root/sqoop_merge/departments/part*

--update
sqoop eval --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username retail_dba \
  --password hadoop \
  --query "update departments set department_name='Testing Merge' where department_id = 9000"

--Insert
sqoop eval --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username retail_dba \
  --password hadoop \
  --query "insert into departments values (10000, 'Inserting for merge')"

sqoop eval --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username retail_dba \
  --password hadoop \
  --query "select * from departments"

--New load
sqoop import \
  --connect "jdbc:mysql://sandbox.hortonworks.com:3306/retail_db" \
  --username=retail_dba \
  --password=hadoop \
  --table departments \
  --as-textfile \
  --target-dir=/user/root/sqoop_merge/departments_delta \
  --where "department_id >= 9000"

hadoop fs -cat /user/root/sqoop_merge/departments_delta/part*

--Merge
sqoop merge --merge-key department_id \
  --new-data /user/root/sqoop_merge/departments_delta \
  --onto /user/root/sqoop_merge/departments \
  --target-dir /user/root/sqoop_merge/departments_stage \
  --class-name departments \
  --jar-file <get_it_from_last_import>

hadoop fs -cat /user/root/sqoop_merge/departments_stage/part*

--Delete old directory
hadoop fs -rm -R /user/root/sqoop_merge/departments

--Move/rename stage directory to original directory
hadoop fs -mv /user/root/sqoop_merge/departments_stage /user/root/sqoop_merge/departments 

--Validate that original directory have merged data
hadoop fs -cat /user/root/sqoop_merge/departments/part*

--Merge process ends
